# combined_model.py
import numpy as np
from sklearn.ensemble import VotingClassifier
from stable_baselines3 import DQN

class CombinedModel:
    def __init__(self, voting_clf, rl_model, env):
        self.voting_clf = voting_clf
        self.rl_model = rl_model
        self.env = env

    def get_rl_predictions(self, X):
        predictions = []
        for i in range(len(X)):
            obs = X[i].reshape(1, -1)  # Ensure observation has the right shape
            action, _states = self.rl_model.predict(obs)
            predictions.append(action)
        return np.array(predictions)

    def combined_predictions(self, X):
        # Get predictions from the VotingClassifier
        voting_preds = self.voting_clf.predict(X)
        print("Voting Pred")
        print(voting_preds)
        # Get RL model predictions
        rl_predictions = self.get_rl_predictions(X)
        print("RL Pred")
        print(rl_predictions)
        # Combined decision logic (example: use RL prediction only if it differs)
        final_predictions = []
        if voting_preds[0] == rl_predictions[0][0]:
            final_predictions.append(voting_preds[0])
        else:
            final_predictions.append(rl_predictions[0][0])
            
        #for vote, rl_pred in zip(voting_preds, rl_predictions):
         #   if vote == rl_pred:
          #      final_predictions.append(vote)  # If both agree, keep the prediction
           # else:
            #    # Custom logic: here we trust the VotingClassifier in case of conflict
                #final_predictions.append(vote)
        
        return np.array(final_predictions)

    def predict(self, X):
        return self.combined_predictions(X)

    def train(self, X_train, y_train):
        self.voting_clf.fit(X_train, y_train)
        self.rl_model.learn(total_timesteps=10000)
